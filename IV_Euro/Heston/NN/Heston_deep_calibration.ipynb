{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Deep Calibration of Heston Model with Neural Network Correction\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "plt.style.use(\"seaborn-v0_8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../Heston_call.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Stage 1: Data loading and preprocessing\u001b[39;00m\n\u001b[32m      2\u001b[39m CSV_PATH = \u001b[33m\"\u001b[39m\u001b[33m../Heston_call.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# set to your dataset path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded shape:\u001b[39m\u001b[33m\"\u001b[39m, df.shape)\n\u001b[32m      6\u001b[39m display(df.head())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/americanOption/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/americanOption/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/americanOption/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/americanOption/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/americanOption/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../Heston_call.csv'"
          ]
        }
      ],
      "source": [
        "# Stage 1: Data loading and preprocessing\n",
        "CSV_PATH = \"../Heston_call.csv\"  # set to your dataset path\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "required_cols = [\"S0\", \"K\", \"T\", \"C_mkt\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "if \"r\" not in df.columns:\n",
        "    df[\"r\"] = 0.02\n",
        "if \"q\" not in df.columns:\n",
        "    df[\"q\"] = 0.0\n",
        "\n",
        "df = df.dropna(subset=[\"S0\", \"K\", \"T\", \"C_mkt\", \"r\", \"q\"]).reset_index(drop=True)\n",
        "df = df[df[\"T\"] > 0.0].reset_index(drop=True)\n",
        "\n",
        "df[\"log_moneyness\"] = np.log(df[\"K\"] / df[\"S0\"])\n",
        "\n",
        "features = [\"log_moneyness\", \"T\", \"r\", \"q\"]\n",
        "X_raw = df[features].values.astype(np.float32)\n",
        "y_prices = df[\"C_mkt\"].values.astype(np.float32)\n",
        "\n",
        "X_mean = X_raw.mean(axis=0, keepdims=True)\n",
        "X_std = X_raw.std(axis=0, keepdims=True) + 1e-8\n",
        "X_norm = (X_raw - X_mean) / X_std\n",
        "\n",
        "X_tensor = torch.from_numpy(X_norm).to(torch.float32)\n",
        "T_tensor = torch.from_numpy(df[\"T\"].values.astype(np.float64))\n",
        "K_tensor = torch.from_numpy(df[\"K\"].values.astype(np.float64))\n",
        "S0_tensor = torch.from_numpy(df[\"S0\"].values.astype(np.float64))\n",
        "r_tensor = torch.from_numpy(df[\"r\"].values.astype(np.float64))\n",
        "q_tensor = torch.from_numpy(df[\"q\"].values.astype(np.float64))\n",
        "y_tensor = torch.from_numpy(y_prices.astype(np.float64))\n",
        "\n",
        "print(f\"Dataset size after cleaning: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 1: Heston characteristic function and semi-analytical pricing\n",
        "INTEGRAL_U_MAX = 60.0\n",
        "INTEGRAL_STEPS = 600\n",
        "DTYPE = torch.float64\n",
        "\n",
        "\n",
        "def heston_cf(u, T, S0, r, q, kappa, theta, sigma, rho, v0):\n",
        "    i = torch.complex(torch.tensor(0.0, dtype=DTYPE, device=u.device),\n",
        "                      torch.tensor(1.0, dtype=DTYPE, device=u.device))\n",
        "    x0 = torch.log(S0)\n",
        "    a = kappa * theta\n",
        "    b = kappa - rho * sigma * i * u\n",
        "    d = torch.sqrt(b * b + (sigma ** 2) * (u * u + i * u))\n",
        "    g = (b - d) / (b + d)\n",
        "    exp_neg_dT = torch.exp(-d * T)\n",
        "    one_minus_g_exp = 1.0 - g * exp_neg_dT\n",
        "    one_minus_g = 1.0 - g\n",
        "    log_term = torch.log(one_minus_g_exp / one_minus_g)\n",
        "    C = (r - q) * i * u * T + (a / (sigma ** 2)) * ((b - d) * T - 2.0 * log_term)\n",
        "    D = ((b - d) / (sigma ** 2)) * ((1.0 - exp_neg_dT) / one_minus_g_exp)\n",
        "    return torch.exp(C + D * v0 + i * u * x0)\n",
        "\n",
        "\n",
        "def heston_call_price(S0, K, T, r, q, kappa, theta, sigma, rho, v0,\n",
        "                      u_max=INTEGRAL_U_MAX, n_u=INTEGRAL_STEPS):\n",
        "    device = S0.device\n",
        "    u = torch.linspace(1e-6, u_max, n_u, device=device, dtype=DTYPE)\n",
        "    u = u.unsqueeze(-1)  # (n_u, 1)\n",
        "    cf1 = heston_cf(u - 1j, T, S0, r, q, kappa, theta, sigma, rho, v0)\n",
        "    cf2 = heston_cf(u, T, S0, r, q, kappa, theta, sigma, rho, v0)\n",
        "    numer1 = torch.exp(-1j * u * torch.log(K)) * cf1\n",
        "    numer2 = torch.exp(-1j * u * torch.log(K)) * cf2\n",
        "    integrand1 = (numer1 / (1j * u)).real\n",
        "    integrand2 = (numer2 / (1j * u)).real\n",
        "    du = u[1] - u[0]\n",
        "    P1 = 0.5 + (du / math.pi) * integrand1.sum(dim=0)\n",
        "    P2 = 0.5 + (du / math.pi) * integrand2.sum(dim=0)\n",
        "    discount_stock = torch.exp(-q * T)\n",
        "    discount_strike = torch.exp(-r * T)\n",
        "    call = S0 * discount_stock * P1 - K * discount_strike * P2\n",
        "    return call.real\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Calibration RMSE: 86447.610331\n",
            "Epoch 010 | Calibration RMSE: 78579.250728\n",
            "Epoch 020 | Calibration RMSE: 69684.098061\n",
            "Epoch 030 | Calibration RMSE: 60744.664967\n",
            "Epoch 040 | Calibration RMSE: 53399.520696\n",
            "Epoch 050 | Calibration RMSE: 48090.888596\n",
            "Epoch 060 | Calibration RMSE: 44658.174407\n",
            "Epoch 070 | Calibration RMSE: 42277.060922\n",
            "Epoch 080 | Calibration RMSE: 40453.579204\n",
            "Epoch 090 | Calibration RMSE: 39038.485182\n",
            "Epoch 100 | Calibration RMSE: 37880.988210\n",
            "Epoch 110 | Calibration RMSE: 36910.970931\n",
            "Epoch 120 | Calibration RMSE: 36080.710106\n",
            "Epoch 130 | Calibration RMSE: 35357.997290\n",
            "Epoch 140 | Calibration RMSE: 34720.454363\n",
            "Epoch 150 | Calibration RMSE: 34151.679191\n",
            "Epoch 160 | Calibration RMSE: 33639.514486\n",
            "Epoch 170 | Calibration RMSE: 33174.620438\n",
            "Epoch 180 | Calibration RMSE: 32749.705185\n",
            "Epoch 190 | Calibration RMSE: 32358.963965\n",
            "Epoch 200 | Calibration RMSE: 31997.712869\n",
            "Calibrated parameters:\n",
            "kappa=7.3171, theta=0.0210, sigma=6.8394, rho=0.9955, v0=4.8442\n"
          ]
        }
      ],
      "source": [
        "# Stage 1: Global calibration via learnable parameters\n",
        "class HestonCalibrator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.kappa_raw = nn.Parameter(torch.tensor(0.5, dtype=DTYPE))\n",
        "        self.theta_raw = nn.Parameter(torch.tensor(0.04, dtype=DTYPE))\n",
        "        self.sigma_raw = nn.Parameter(torch.tensor(0.3, dtype=DTYPE))\n",
        "        self.rho_raw = nn.Parameter(torch.tensor(-0.1, dtype=DTYPE))\n",
        "        self.v0_raw = nn.Parameter(torch.tensor(0.04, dtype=DTYPE))\n",
        "\n",
        "    def forward(self):\n",
        "        kappa = torch.nn.functional.softplus(self.kappa_raw)\n",
        "        theta = torch.nn.functional.softplus(self.theta_raw)\n",
        "        sigma = torch.nn.functional.softplus(self.sigma_raw)\n",
        "        rho = torch.tanh(self.rho_raw)\n",
        "        v0 = torch.nn.functional.softplus(self.v0_raw)\n",
        "        return kappa, theta, sigma, rho, v0\n",
        "\n",
        "\n",
        "def rmse_loss(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2) + 1e-8)\n",
        "\n",
        "calibrator = HestonCalibrator().to(device)\n",
        "optimizer = optim.Adam(calibrator.parameters(), lr=5e-2)\n",
        "num_epochs = 200\n",
        "\n",
        "T_batch = T_tensor.to(device)\n",
        "K_batch = K_tensor.to(device)\n",
        "S0_batch = S0_tensor.to(device)\n",
        "r_batch = r_tensor.to(device)\n",
        "q_batch = q_tensor.to(device)\n",
        "y_batch = y_tensor.to(device)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    params = calibrator()\n",
        "    price_pred = heston_call_price(S0_batch, K_batch, T_batch, r_batch, q_batch, *params)\n",
        "    loss = rmse_loss(price_pred, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Calibration RMSE: {loss.item():.6f}\")\n",
        "\n",
        "kappa_star, theta_star, sigma_star, rho_star, v0_star = [p.detach().clone() for p in calibrator()]\n",
        "print(\"Calibrated parameters:\")\n",
        "print(f\"kappa={kappa_star.item():.4f}, theta={theta_star.item():.4f}, sigma={sigma_star.item():.4f}, rho={rho_star.item():.4f}, v0={v0_star.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Heston RMSE: 31963.055934\n"
          ]
        }
      ],
      "source": [
        "# Stage 2: Baseline Heston prices and residuals\n",
        "with torch.no_grad():\n",
        "    C_heston = heston_call_price(S0_batch, K_batch, T_batch, r_batch, q_batch,\n",
        "                                 kappa_star, theta_star, sigma_star, rho_star, v0_star)\n",
        "\n",
        "C_heston_np = C_heston.cpu().numpy()\n",
        "residual = y_prices - C_heston_np\n",
        "\n",
        "df[\"C_heston\"] = C_heston_np\n",
        "df[\"residual\"] = residual\n",
        "\n",
        "baseline_rmse = np.sqrt(np.mean((C_heston_np - y_prices) ** 2))\n",
        "print(f\"Baseline Heston RMSE: {baseline_rmse:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Train RMSE: 30854.336719 | Val RMSE: 32413.561415\n",
            "Epoch 020 | Train RMSE: 24599.669029 | Val RMSE: 25173.185547\n",
            "Epoch 040 | Train RMSE: 16510.442215 | Val RMSE: 17226.534397\n",
            "Epoch 060 | Train RMSE: 15446.052902 | Val RMSE: 16444.409071\n",
            "Epoch 080 | Train RMSE: 14940.844699 | Val RMSE: 15677.548503\n",
            "Epoch 100 | Train RMSE: 13782.846624 | Val RMSE: 14971.727539\n",
            "Epoch 120 | Train RMSE: 13150.771819 | Val RMSE: 14112.367296\n",
            "Epoch 140 | Train RMSE: 12517.972545 | Val RMSE: 13436.052626\n",
            "Epoch 160 | Train RMSE: 11868.493471 | Val RMSE: 12924.819010\n",
            "Epoch 180 | Train RMSE: 11444.563588 | Val RMSE: 12486.921875\n",
            "Epoch 200 | Train RMSE: 11136.032059 | Val RMSE: 12078.605035\n",
            "Epoch 220 | Train RMSE: 10759.199477 | Val RMSE: 11704.506402\n",
            "Epoch 240 | Train RMSE: 10429.088184 | Val RMSE: 11342.123589\n",
            "Epoch 260 | Train RMSE: 10643.606027 | Val RMSE: 10995.494249\n",
            "Epoch 280 | Train RMSE: 9859.470006 | Val RMSE: 10639.432617\n",
            "Epoch 300 | Train RMSE: 9642.437612 | Val RMSE: 10271.061632\n",
            "Epoch 320 | Train RMSE: 9423.667704 | Val RMSE: 9932.583388\n",
            "Epoch 340 | Train RMSE: 8886.911789 | Val RMSE: 9575.824653\n",
            "Epoch 360 | Train RMSE: 8604.802902 | Val RMSE: 9214.202040\n",
            "Epoch 380 | Train RMSE: 8371.879185 | Val RMSE: 8858.940647\n",
            "Epoch 400 | Train RMSE: 7874.174477 | Val RMSE: 8452.346680\n",
            "Epoch 420 | Train RMSE: 7546.597914 | Val RMSE: 8038.399631\n",
            "Epoch 440 | Train RMSE: 7081.607401 | Val RMSE: 7633.758952\n",
            "Epoch 460 | Train RMSE: 6715.203390 | Val RMSE: 7237.726454\n",
            "Epoch 480 | Train RMSE: 6547.858524 | Val RMSE: 6893.216037\n",
            "Epoch 500 | Train RMSE: 6310.836984 | Val RMSE: 6553.734863\n",
            "Epoch 520 | Train RMSE: 5779.570850 | Val RMSE: 6207.972168\n",
            "Epoch 540 | Train RMSE: 5545.077546 | Val RMSE: 5864.315891\n",
            "Epoch 560 | Train RMSE: 5217.459242 | Val RMSE: 5540.165337\n",
            "Epoch 580 | Train RMSE: 4762.585303 | Val RMSE: 5279.119683\n",
            "Epoch 600 | Train RMSE: 4559.492477 | Val RMSE: 5040.528971\n",
            "Epoch 620 | Train RMSE: 4475.339453 | Val RMSE: 4789.373942\n",
            "Epoch 640 | Train RMSE: 4303.942780 | Val RMSE: 4607.009847\n",
            "Epoch 660 | Train RMSE: 4088.560456 | Val RMSE: 4419.263373\n",
            "Epoch 680 | Train RMSE: 3930.743433 | Val RMSE: 4320.667657\n",
            "Epoch 700 | Train RMSE: 3898.477166 | Val RMSE: 4149.216865\n",
            "Epoch 720 | Train RMSE: 3731.711063 | Val RMSE: 4051.054877\n",
            "Epoch 740 | Train RMSE: 3704.730326 | Val RMSE: 3937.456394\n",
            "Epoch 760 | Train RMSE: 3553.814579 | Val RMSE: 3818.683743\n",
            "Epoch 780 | Train RMSE: 3377.853833 | Val RMSE: 3756.991496\n",
            "Epoch 800 | Train RMSE: 3243.159145 | Val RMSE: 3647.693129\n",
            "Epoch 820 | Train RMSE: 3197.569158 | Val RMSE: 3584.872247\n",
            "Epoch 840 | Train RMSE: 3129.404374 | Val RMSE: 3474.826009\n",
            "Epoch 860 | Train RMSE: 3059.729789 | Val RMSE: 3378.409627\n",
            "Epoch 880 | Train RMSE: 2956.646434 | Val RMSE: 3278.546672\n",
            "Epoch 900 | Train RMSE: 2857.008850 | Val RMSE: 3199.764723\n",
            "Epoch 920 | Train RMSE: 2781.345065 | Val RMSE: 3175.663405\n",
            "Epoch 940 | Train RMSE: 2676.535185 | Val RMSE: 3044.995761\n",
            "Epoch 960 | Train RMSE: 2698.707431 | Val RMSE: 2959.545492\n",
            "Epoch 980 | Train RMSE: 2620.184820 | Val RMSE: 2954.497403\n",
            "Epoch 1000 | Train RMSE: 2589.804283 | Val RMSE: 2856.929213\n",
            "Epoch 1020 | Train RMSE: 2470.629952 | Val RMSE: 2784.203369\n",
            "Epoch 1040 | Train RMSE: 2487.329735 | Val RMSE: 2755.097724\n",
            "Epoch 1060 | Train RMSE: 2383.018846 | Val RMSE: 2696.208849\n",
            "Epoch 1080 | Train RMSE: 2345.671238 | Val RMSE: 2625.095188\n",
            "Epoch 1100 | Train RMSE: 2266.778247 | Val RMSE: 2545.037198\n",
            "Epoch 1120 | Train RMSE: 2229.015959 | Val RMSE: 2469.027954\n",
            "Epoch 1140 | Train RMSE: 2178.853414 | Val RMSE: 2394.608168\n",
            "Epoch 1160 | Train RMSE: 2152.313669 | Val RMSE: 2357.338860\n",
            "Epoch 1180 | Train RMSE: 2085.019754 | Val RMSE: 2302.263245\n",
            "Epoch 1200 | Train RMSE: 2047.831109 | Val RMSE: 2255.095330\n",
            "Epoch 1220 | Train RMSE: 1974.489631 | Val RMSE: 2200.523817\n",
            "Epoch 1240 | Train RMSE: 1920.120415 | Val RMSE: 2184.938416\n",
            "Epoch 1260 | Train RMSE: 1908.643218 | Val RMSE: 2138.848253\n",
            "Epoch 1280 | Train RMSE: 1837.208743 | Val RMSE: 2059.075060\n",
            "Epoch 1300 | Train RMSE: 1814.581353 | Val RMSE: 2018.851766\n",
            "Epoch 1320 | Train RMSE: 1831.324703 | Val RMSE: 2020.001628\n",
            "Epoch 1340 | Train RMSE: 1757.190879 | Val RMSE: 1936.771952\n",
            "Epoch 1360 | Train RMSE: 1751.824783 | Val RMSE: 1892.557726\n",
            "Epoch 1380 | Train RMSE: 1729.079269 | Val RMSE: 1854.515015\n",
            "Epoch 1400 | Train RMSE: 1726.912905 | Val RMSE: 1817.557380\n",
            "Epoch 1420 | Train RMSE: 1641.593702 | Val RMSE: 1770.610975\n",
            "Epoch 1440 | Train RMSE: 1668.920339 | Val RMSE: 1786.530755\n",
            "Epoch 1460 | Train RMSE: 1569.026527 | Val RMSE: 1706.326809\n",
            "Epoch 1480 | Train RMSE: 1574.383715 | Val RMSE: 1691.204488\n",
            "Epoch 1500 | Train RMSE: 1548.323893 | Val RMSE: 1686.465983\n",
            "Epoch 1520 | Train RMSE: 1573.725204 | Val RMSE: 1627.154114\n",
            "Epoch 1540 | Train RMSE: 1513.782206 | Val RMSE: 1646.088962\n",
            "Epoch 1560 | Train RMSE: 1474.233383 | Val RMSE: 1603.800903\n",
            "Epoch 1580 | Train RMSE: 1464.228101 | Val RMSE: 1589.241923\n",
            "Epoch 1600 | Train RMSE: 1394.924210 | Val RMSE: 1530.113159\n",
            "Epoch 1620 | Train RMSE: 1406.376977 | Val RMSE: 1539.908712\n",
            "Epoch 1640 | Train RMSE: 1419.022806 | Val RMSE: 1514.915961\n",
            "Epoch 1660 | Train RMSE: 1348.523792 | Val RMSE: 1494.707669\n",
            "Epoch 1680 | Train RMSE: 1354.582221 | Val RMSE: 1458.031698\n",
            "Epoch 1700 | Train RMSE: 1281.122209 | Val RMSE: 1458.395474\n",
            "Epoch 1720 | Train RMSE: 1339.052484 | Val RMSE: 1441.227587\n",
            "Epoch 1740 | Train RMSE: 1284.714132 | Val RMSE: 1419.777629\n",
            "Epoch 1760 | Train RMSE: 1257.289097 | Val RMSE: 1397.552110\n",
            "Epoch 1780 | Train RMSE: 1250.360145 | Val RMSE: 1361.622606\n",
            "Epoch 1800 | Train RMSE: 1220.027699 | Val RMSE: 1357.513611\n",
            "Epoch 1820 | Train RMSE: 1147.556075 | Val RMSE: 1351.926731\n",
            "Epoch 1840 | Train RMSE: 1179.863436 | Val RMSE: 1332.300486\n",
            "Epoch 1860 | Train RMSE: 1228.764177 | Val RMSE: 1330.316301\n",
            "Epoch 1880 | Train RMSE: 1195.879175 | Val RMSE: 1305.063100\n",
            "Epoch 1900 | Train RMSE: 1097.497605 | Val RMSE: 1280.709578\n",
            "Epoch 1920 | Train RMSE: 1098.481350 | Val RMSE: 1306.122352\n",
            "Epoch 1940 | Train RMSE: 1043.244204 | Val RMSE: 1252.259040\n",
            "Epoch 1960 | Train RMSE: 1136.424769 | Val RMSE: 1294.142361\n",
            "Epoch 1980 | Train RMSE: 1104.503235 | Val RMSE: 1251.935703\n",
            "Epoch 2000 | Train RMSE: 1046.338624 | Val RMSE: 1236.209371\n",
            "Epoch 2020 | Train RMSE: 1003.316318 | Val RMSE: 1200.435940\n",
            "Epoch 2040 | Train RMSE: 975.521572 | Val RMSE: 1195.672716\n",
            "Epoch 2060 | Train RMSE: 1021.263569 | Val RMSE: 1171.017297\n",
            "Epoch 2080 | Train RMSE: 1053.575140 | Val RMSE: 1219.309055\n",
            "Epoch 2100 | Train RMSE: 1010.202753 | Val RMSE: 1188.601600\n",
            "Epoch 2120 | Train RMSE: 982.589173 | Val RMSE: 1178.522980\n",
            "Epoch 2140 | Train RMSE: 1012.411714 | Val RMSE: 1128.028714\n",
            "Epoch 2160 | Train RMSE: 919.965771 | Val RMSE: 1121.736379\n",
            "Epoch 2180 | Train RMSE: 981.034644 | Val RMSE: 1159.268979\n",
            "Epoch 2200 | Train RMSE: 1012.788843 | Val RMSE: 1091.644338\n",
            "Epoch 2220 | Train RMSE: 831.040668 | Val RMSE: 1087.711178\n",
            "Epoch 2240 | Train RMSE: 913.792685 | Val RMSE: 1087.429494\n",
            "Epoch 2260 | Train RMSE: 961.855369 | Val RMSE: 1061.480893\n",
            "Epoch 2280 | Train RMSE: 914.579638 | Val RMSE: 1069.753686\n",
            "Epoch 2300 | Train RMSE: 859.825202 | Val RMSE: 1052.212294\n",
            "Epoch 2320 | Train RMSE: 882.188655 | Val RMSE: 1101.431949\n",
            "Epoch 2340 | Train RMSE: 884.014373 | Val RMSE: 1059.423933\n",
            "Epoch 2360 | Train RMSE: 856.431317 | Val RMSE: 1010.905799\n",
            "Epoch 2380 | Train RMSE: 845.525140 | Val RMSE: 1037.338230\n",
            "Epoch 2400 | Train RMSE: 872.622815 | Val RMSE: 1007.366764\n",
            "Epoch 2420 | Train RMSE: 818.638654 | Val RMSE: 984.614343\n",
            "Epoch 2440 | Train RMSE: 851.589522 | Val RMSE: 1006.848514\n",
            "Epoch 2460 | Train RMSE: 843.614146 | Val RMSE: 967.823832\n",
            "Epoch 2480 | Train RMSE: 803.132324 | Val RMSE: 960.038130\n",
            "Epoch 2500 | Train RMSE: 856.405344 | Val RMSE: 995.369839\n",
            "Epoch 2520 | Train RMSE: 754.705343 | Val RMSE: 941.883114\n",
            "Epoch 2540 | Train RMSE: 743.567483 | Val RMSE: 958.250431\n",
            "Epoch 2560 | Train RMSE: 735.381537 | Val RMSE: 944.744968\n",
            "Epoch 2580 | Train RMSE: 833.884083 | Val RMSE: 959.805976\n",
            "Epoch 2600 | Train RMSE: 826.007289 | Val RMSE: 949.423726\n",
            "Epoch 2620 | Train RMSE: 763.681871 | Val RMSE: 916.432146\n",
            "Epoch 2640 | Train RMSE: 786.291174 | Val RMSE: 944.400530\n",
            "Epoch 2660 | Train RMSE: 756.271324 | Val RMSE: 950.107049\n",
            "Epoch 2680 | Train RMSE: 725.733508 | Val RMSE: 947.867906\n",
            "Epoch 2700 | Train RMSE: 690.766223 | Val RMSE: 933.321855\n",
            "Epoch 2720 | Train RMSE: 712.183405 | Val RMSE: 913.250892\n",
            "Epoch 2740 | Train RMSE: 726.768758 | Val RMSE: 914.166036\n",
            "Epoch 2760 | Train RMSE: 738.518331 | Val RMSE: 947.691979\n",
            "Epoch 2780 | Train RMSE: 735.122097 | Val RMSE: 903.278497\n",
            "Epoch 2800 | Train RMSE: 698.281430 | Val RMSE: 876.486179\n",
            "Epoch 2820 | Train RMSE: 682.762527 | Val RMSE: 862.115848\n",
            "Epoch 2840 | Train RMSE: 660.992227 | Val RMSE: 893.193793\n",
            "Epoch 2860 | Train RMSE: 597.035372 | Val RMSE: 869.382385\n",
            "Epoch 2880 | Train RMSE: 668.426329 | Val RMSE: 910.238183\n",
            "Epoch 2900 | Train RMSE: 646.649489 | Val RMSE: 849.338267\n",
            "Epoch 2920 | Train RMSE: 739.612834 | Val RMSE: 847.741747\n",
            "Epoch 2940 | Train RMSE: 661.693498 | Val RMSE: 856.174452\n",
            "Epoch 2960 | Train RMSE: 589.423588 | Val RMSE: 835.040856\n",
            "Epoch 2980 | Train RMSE: 682.199119 | Val RMSE: 847.887778\n",
            "Epoch 3000 | Train RMSE: 637.648884 | Val RMSE: 814.929135\n",
            "Epoch 3020 | Train RMSE: 638.232584 | Val RMSE: 832.868266\n",
            "Epoch 3040 | Train RMSE: 636.890272 | Val RMSE: 844.900821\n",
            "Epoch 3060 | Train RMSE: 631.939321 | Val RMSE: 850.355515\n",
            "Epoch 3080 | Train RMSE: 589.133081 | Val RMSE: 811.527890\n",
            "Epoch 3100 | Train RMSE: 561.908762 | Val RMSE: 816.300329\n",
            "Epoch 3120 | Train RMSE: 606.948306 | Val RMSE: 828.666931\n",
            "Epoch 3140 | Train RMSE: 584.981823 | Val RMSE: 881.491859\n",
            "Epoch 3160 | Train RMSE: 597.644037 | Val RMSE: 861.850189\n",
            "Epoch 3180 | Train RMSE: 550.558054 | Val RMSE: 797.709234\n",
            "Epoch 3200 | Train RMSE: 527.278313 | Val RMSE: 803.743018\n",
            "Epoch 3220 | Train RMSE: 548.861532 | Val RMSE: 806.934733\n",
            "Epoch 3240 | Train RMSE: 569.973099 | Val RMSE: 859.957862\n",
            "Epoch 3260 | Train RMSE: 541.228398 | Val RMSE: 796.549798\n",
            "Epoch 3280 | Train RMSE: 577.684794 | Val RMSE: 846.386671\n",
            "Epoch 3300 | Train RMSE: 585.196546 | Val RMSE: 803.057048\n",
            "Epoch 3320 | Train RMSE: 519.060084 | Val RMSE: 838.591363\n",
            "Epoch 3340 | Train RMSE: 553.726382 | Val RMSE: 816.615094\n",
            "Epoch 3360 | Train RMSE: 491.130788 | Val RMSE: 748.132067\n",
            "Epoch 3380 | Train RMSE: 522.373991 | Val RMSE: 760.951940\n",
            "Epoch 3400 | Train RMSE: 579.245359 | Val RMSE: 772.064635\n",
            "Epoch 3420 | Train RMSE: 503.978828 | Val RMSE: 779.769645\n",
            "Epoch 3440 | Train RMSE: 498.886995 | Val RMSE: 771.572637\n",
            "Epoch 3460 | Train RMSE: 526.481063 | Val RMSE: 788.319143\n",
            "Epoch 3480 | Train RMSE: 516.098067 | Val RMSE: 861.611037\n",
            "Epoch 3500 | Train RMSE: 538.207585 | Val RMSE: 817.650143\n",
            "Epoch 3520 | Train RMSE: 488.670599 | Val RMSE: 798.644704\n",
            "Epoch 3540 | Train RMSE: 467.346414 | Val RMSE: 751.473679\n",
            "Epoch 3560 | Train RMSE: 552.651180 | Val RMSE: 752.644653\n",
            "Epoch 3580 | Train RMSE: 501.569690 | Val RMSE: 777.711558\n",
            "Epoch 3600 | Train RMSE: 493.062714 | Val RMSE: 903.136634\n",
            "Epoch 3620 | Train RMSE: 485.396951 | Val RMSE: 746.749107\n",
            "Epoch 3640 | Train RMSE: 475.468822 | Val RMSE: 740.861008\n",
            "Epoch 3660 | Train RMSE: 479.622246 | Val RMSE: 725.322081\n",
            "Epoch 3680 | Train RMSE: 449.668170 | Val RMSE: 774.725847\n",
            "Epoch 3700 | Train RMSE: 475.570272 | Val RMSE: 744.070379\n",
            "Epoch 3720 | Train RMSE: 434.475838 | Val RMSE: 759.607920\n",
            "Epoch 3740 | Train RMSE: 470.353697 | Val RMSE: 803.447394\n",
            "Epoch 3760 | Train RMSE: 448.179912 | Val RMSE: 748.589459\n",
            "Epoch 3780 | Train RMSE: 551.457145 | Val RMSE: 737.941884\n",
            "Epoch 3800 | Train RMSE: 436.578330 | Val RMSE: 817.801938\n",
            "Epoch 3820 | Train RMSE: 434.257698 | Val RMSE: 769.301971\n",
            "Epoch 3840 | Train RMSE: 422.915064 | Val RMSE: 770.930376\n",
            "Epoch 3860 | Train RMSE: 416.761528 | Val RMSE: 758.978138\n",
            "Epoch 3880 | Train RMSE: 433.866285 | Val RMSE: 726.814177\n",
            "Epoch 3900 | Train RMSE: 442.776899 | Val RMSE: 699.796709\n",
            "Epoch 3920 | Train RMSE: 401.204610 | Val RMSE: 729.894524\n",
            "Epoch 3940 | Train RMSE: 388.163101 | Val RMSE: 811.692478\n",
            "Epoch 3960 | Train RMSE: 401.687982 | Val RMSE: 757.814284\n",
            "Epoch 3980 | Train RMSE: 400.130731 | Val RMSE: 722.321001\n",
            "Epoch 4000 | Train RMSE: 405.353228 | Val RMSE: 706.028771\n",
            "Epoch 4020 | Train RMSE: 407.099671 | Val RMSE: 721.345707\n",
            "Epoch 4040 | Train RMSE: 416.936138 | Val RMSE: 665.669763\n",
            "Epoch 4060 | Train RMSE: 392.296934 | Val RMSE: 697.371557\n",
            "Epoch 4080 | Train RMSE: 401.680645 | Val RMSE: 699.703664\n",
            "Epoch 4100 | Train RMSE: 372.043502 | Val RMSE: 710.769787\n",
            "Epoch 4120 | Train RMSE: 376.297799 | Val RMSE: 812.111481\n",
            "Epoch 4140 | Train RMSE: 397.504887 | Val RMSE: 676.175829\n",
            "Epoch 4160 | Train RMSE: 375.407332 | Val RMSE: 697.989980\n",
            "Epoch 4180 | Train RMSE: 336.410260 | Val RMSE: 716.511509\n",
            "Epoch 4200 | Train RMSE: 367.789127 | Val RMSE: 686.573198\n",
            "Epoch 4220 | Train RMSE: 379.557113 | Val RMSE: 689.722375\n",
            "Epoch 4240 | Train RMSE: 431.221803 | Val RMSE: 666.952247\n",
            "Epoch 4260 | Train RMSE: 384.611370 | Val RMSE: 675.190789\n",
            "Epoch 4280 | Train RMSE: 344.940829 | Val RMSE: 730.144589\n",
            "Epoch 4300 | Train RMSE: 314.934005 | Val RMSE: 667.433851\n",
            "Epoch 4320 | Train RMSE: 349.121769 | Val RMSE: 671.526232\n",
            "Epoch 4340 | Train RMSE: 399.674239 | Val RMSE: 642.396488\n",
            "Epoch 4360 | Train RMSE: 360.111134 | Val RMSE: 674.364761\n",
            "Epoch 4380 | Train RMSE: 336.163366 | Val RMSE: 677.305186\n",
            "Epoch 4400 | Train RMSE: 338.839051 | Val RMSE: 661.327533\n",
            "Epoch 4420 | Train RMSE: 321.932172 | Val RMSE: 687.402513\n",
            "Epoch 4440 | Train RMSE: 324.646899 | Val RMSE: 652.316250\n",
            "Epoch 4460 | Train RMSE: 325.471534 | Val RMSE: 644.804240\n",
            "Epoch 4480 | Train RMSE: 296.533304 | Val RMSE: 644.908223\n",
            "Epoch 4500 | Train RMSE: 319.047805 | Val RMSE: 621.856547\n",
            "Epoch 4520 | Train RMSE: 313.244974 | Val RMSE: 615.031898\n",
            "Epoch 4540 | Train RMSE: 363.747848 | Val RMSE: 661.618364\n",
            "Epoch 4560 | Train RMSE: 337.199161 | Val RMSE: 656.199787\n",
            "Epoch 4580 | Train RMSE: 319.857948 | Val RMSE: 655.122865\n",
            "Epoch 4600 | Train RMSE: 294.946247 | Val RMSE: 698.282603\n",
            "Epoch 4620 | Train RMSE: 290.505624 | Val RMSE: 624.149906\n",
            "Epoch 4640 | Train RMSE: 308.772323 | Val RMSE: 668.070562\n",
            "Epoch 4660 | Train RMSE: 298.275377 | Val RMSE: 607.079610\n",
            "Epoch 4680 | Train RMSE: 293.732964 | Val RMSE: 643.211517\n",
            "Epoch 4700 | Train RMSE: 286.159300 | Val RMSE: 643.379768\n",
            "Epoch 4720 | Train RMSE: 305.485657 | Val RMSE: 588.348753\n",
            "Epoch 4740 | Train RMSE: 349.052134 | Val RMSE: 577.880264\n",
            "Epoch 4760 | Train RMSE: 280.300632 | Val RMSE: 654.578766\n",
            "Epoch 4780 | Train RMSE: 288.616709 | Val RMSE: 595.852698\n",
            "Epoch 4800 | Train RMSE: 276.985573 | Val RMSE: 582.639381\n",
            "Epoch 4820 | Train RMSE: 271.446392 | Val RMSE: 599.180479\n",
            "Epoch 4840 | Train RMSE: 294.993050 | Val RMSE: 598.092233\n",
            "Epoch 4860 | Train RMSE: 278.178299 | Val RMSE: 587.521488\n",
            "Epoch 4880 | Train RMSE: 279.580805 | Val RMSE: 610.934547\n",
            "Epoch 4900 | Train RMSE: 290.031449 | Val RMSE: 629.194448\n",
            "Epoch 4920 | Train RMSE: 276.259910 | Val RMSE: 570.863544\n",
            "Epoch 4940 | Train RMSE: 265.190889 | Val RMSE: 589.050598\n",
            "Epoch 4960 | Train RMSE: 256.073664 | Val RMSE: 542.350362\n",
            "Epoch 4980 | Train RMSE: 278.392661 | Val RMSE: 572.215396\n",
            "Epoch 5000 | Train RMSE: 278.424030 | Val RMSE: 578.740889\n"
          ]
        }
      ],
      "source": [
        "# Stage 3: Neural network learning the correction Î”C\n",
        "class ResidualDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "X_residual = torch.from_numpy(X_norm).to(torch.float32)\n",
        "y_residual = torch.from_numpy(residual.astype(np.float32)).unsqueeze(-1)\n",
        "\n",
        "full_dataset = ResidualDataset(X_residual, y_residual)\n",
        "dataset_size = len(full_dataset)\n",
        "val_size = max(1, int(0.2 * dataset_size))\n",
        "train_size = dataset_size - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "batch_size = min(64, train_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class ResidualNet(nn.Module):\n",
        "    def __init__(self, input_dim=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "residual_model = ResidualNet(input_dim=X_residual.shape[1]).to(device)\n",
        "optimizer_res = optim.AdamW(residual_model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "num_epochs_nn = 5000\n",
        "\n",
        "for epoch in range(1, num_epochs_nn + 1):\n",
        "    residual_model.train()\n",
        "    train_losses = []\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer_res.zero_grad()\n",
        "        preds = residual_model(batch_X)\n",
        "        loss = rmse_loss(preds, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer_res.step()\n",
        "        train_losses.append(loss.item())\n",
        "    train_rmse = float(np.mean(train_losses))\n",
        "\n",
        "    residual_model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            preds = residual_model(batch_X)\n",
        "            val_losses.append(rmse_loss(preds, batch_y).item())\n",
        "    val_rmse = float(np.mean(val_losses))\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Train RMSE: {train_rmse:.6f} | Val RMSE: {val_rmse:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Heston RMSE: 31963.055934\n",
            "NN-corrected RMSE: 481.299399\n",
            "Improvement: 31481.756535\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>K</th>\n",
              "      <th>T</th>\n",
              "      <th>C_mkt</th>\n",
              "      <th>C_heston</th>\n",
              "      <th>residual</th>\n",
              "      <th>deltaC_pred</th>\n",
              "      <th>C_final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570.0</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>93.78</td>\n",
              "      <td>150500.316295</td>\n",
              "      <td>-150406.536296</td>\n",
              "      <td>-138671.687500</td>\n",
              "      <td>11828.628795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>575.0</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>102.28</td>\n",
              "      <td>144125.830410</td>\n",
              "      <td>-144023.550411</td>\n",
              "      <td>-133091.265625</td>\n",
              "      <td>11034.564785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>580.0</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>85.62</td>\n",
              "      <td>137533.113171</td>\n",
              "      <td>-137447.493169</td>\n",
              "      <td>-126942.250000</td>\n",
              "      <td>10590.863171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>590.0</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>75.65</td>\n",
              "      <td>123758.350983</td>\n",
              "      <td>-123682.700981</td>\n",
              "      <td>-114591.265625</td>\n",
              "      <td>9167.085358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>600.0</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>76.35</td>\n",
              "      <td>109314.236642</td>\n",
              "      <td>-109237.886644</td>\n",
              "      <td>-102600.625000</td>\n",
              "      <td>6713.611642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       K         T   C_mkt       C_heston       residual    deltaC_pred  \\\n",
              "0  570.0  0.006484   93.78  150500.316295 -150406.536296 -138671.687500   \n",
              "1  575.0  0.006484  102.28  144125.830410 -144023.550411 -133091.265625   \n",
              "2  580.0  0.006484   85.62  137533.113171 -137447.493169 -126942.250000   \n",
              "3  590.0  0.006484   75.65  123758.350983 -123682.700981 -114591.265625   \n",
              "4  600.0  0.006484   76.35  109314.236642 -109237.886644 -102600.625000   \n",
              "\n",
              "        C_final  \n",
              "0  11828.628795  \n",
              "1  11034.564785  \n",
              "2  10590.863171  \n",
              "3   9167.085358  \n",
              "4   6713.611642  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Stage 4: Final pricing performance\n",
        "residual_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_residual_device = X_residual.to(device)\n",
        "    deltaC_pred = residual_model(X_residual_device).cpu().numpy().flatten()\n",
        "\n",
        "C_final = df[\"C_heston\"].values + deltaC_pred\n",
        "final_rmse = np.sqrt(np.mean((C_final - y_prices) ** 2))\n",
        "improvement = baseline_rmse - final_rmse\n",
        "\n",
        "print(f\"Baseline Heston RMSE: {baseline_rmse:.6f}\")\n",
        "print(f\"NN-corrected RMSE: {final_rmse:.6f}\")\n",
        "print(f\"Improvement: {improvement:.6f}\")\n",
        "\n",
        "df[\"deltaC_pred\"] = deltaC_pred\n",
        "df[\"C_final\"] = C_final\n",
        "\n",
        "cols_to_show = [\"K\", \"T\", \"C_mkt\", \"C_heston\", \"residual\", \"deltaC_pred\", \"C_final\"]\n",
        "display(df[cols_to_show].head())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "americanOption",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
